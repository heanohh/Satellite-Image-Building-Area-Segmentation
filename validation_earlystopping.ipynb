{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-yeM27xLt8K"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K9HAr30DGYR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1657de59-dba6-4f66-c152-3b401b641b10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m637.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.15.2+cu118)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.9.2 (from segmentation-models-pytorch)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.65.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (8.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.1+cu118)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0)\n",
            "Collecting huggingface-hub (from timm==0.9.2->segmentation-models-pytorch)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm==0.9.2->segmentation-models-pytorch)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16427 sha256=7923a1fe6a3b8bfa2de7d64a29f13f011f2bb896ec813ab4b046bf090ab618f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=3c3aef49c426108ec0b6f2615cae2dc4deac17334f9de4888596889ac6c5f575\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: safetensors, munch, huggingface-hub, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 huggingface-hub-0.16.4 munch-4.0.0 pretrainedmodels-0.7.4 safetensors-0.3.1 segmentation-models-pytorch-0.3.3 timm-0.9.2\n"
          ]
        }
      ],
      "source": [
        "%pip install segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VolGYa0kJKqY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from typing import List, Union\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "import argparse\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KnPjwNmLwfR"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "88cEQaQyLhJZ"
      },
      "outputs": [],
      "source": [
        "# RLE 디코딩 함수\n",
        "def rle_decode(mask_rle, shape):\n",
        "    s = mask_rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img.reshape(shape)\n",
        "\n",
        "# RLE 인코딩 함수\n",
        "def rle_encode(mask):\n",
        "    pixels = mask.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVVsT126Lsvq"
      },
      "source": [
        "## Custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "e4NSUOIOL7RA"
      },
      "outputs": [],
      "source": [
        "class SatelliteDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None, infer=False, subset_indices=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.transform = transform\n",
        "        self.infer = infer\n",
        "        self.subset_indices = subset_indices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset_indices) if self.subset_indices else len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.subset_indices:\n",
        "           idx = self.subset_indices[idx]\n",
        "\n",
        "        img_path = self.data['img_path'][idx]\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.infer:\n",
        "            if self.transform:\n",
        "                image = self.transform(image=image)['image']\n",
        "            return image\n",
        "\n",
        "        mask_rle = self.data['mask_rle'][idx]\n",
        "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        return image, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADVSIlHVMAfA"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BtSXBQydRdEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd4499b-eb6e-422b-e592-1a64f1afb7ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/ai project 4-1\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "%cd /content/drive/MyDrive/ai project 4-1\n",
        "#데이터있는 주소\n",
        "#!unzip -qq \"/content/drive/MyDrive/ai project 4-1/data.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "dLAKZBXaMDf5"
      },
      "outputs": [],
      "source": [
        "\n",
        "transform = A.Compose(\n",
        "    [\n",
        "        A.CenterCrop(224, 224, p=0.1),\n",
        "\n",
        "        #A.HorizontalFlip(p = 0.5),\n",
        "\n",
        "        #A.IAAAdditiveGaussianNoise(p=0.2),\n",
        "        #A.IAAPerspective(p=0.5),\n",
        "\n",
        "        #A.OneOf([\n",
        "            #A.CLAHE(p=1),\n",
        "            #A.RandomBrightness(p = 1),\n",
        "            #A.RandomGamma(p = 1)\n",
        "        #], p = 0.5),\n",
        "\n",
        "        #A.OneOf([\n",
        "            #A.IAASharpen(p = 1),\n",
        "            #A.Blur(blur_limit=3, p=1),\n",
        "            #A.GaussianBlur(p = 1),\n",
        "            #A.MotionBlur(blur_limit=3, p=1),\n",
        "            #A.GaussNoise(p = 1)\n",
        "        #], p = 0.5),\n",
        "\n",
        "        #A.OneOf([\n",
        "            #A.RandomContrast(p=1),\n",
        "            #A.HueSaturationValue(p=1),\n",
        "        #], p = 0.5),\n",
        "\n",
        "        A.Resize(224, 224),\n",
        "\n",
        "        A.Normalize(),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "transform_test = A.Compose(\n",
        "    [\n",
        "        A.Resize(224, 224),\n",
        "\n",
        "        A.Normalize(),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "#dataset = SatelliteDataset(csv_file='train.csv', transform=transform)\n",
        "#dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##train-validation 데이터 분할"
      ],
      "metadata": {
        "id": "L1Km2Gawkkzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train data를 train_rate 의 비율만큼 *분할함*.\n",
        "\n",
        "단 transform이 train, val 둘다 적용됨"
      ],
      "metadata": {
        "id": "q9p5yn4IihQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_rate=0.9\n",
        "train_val_dataset =  SatelliteDataset(csv_file='./train.csv', transform=None)\n",
        "\n",
        "train_size = int(train_rate * len(train_val_dataset))\n",
        "valid_size = len(train_val_dataset) - train_size\n",
        "\n",
        "dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [train_size, valid_size])\n",
        "\n",
        "data = SatelliteDataset(csv_file='./train.csv', transform=transform, subset_indices= dataset.indices)\n",
        "val_data = SatelliteDataset(csv_file='./train.csv', transform=transform_test,subset_indices=val_dataset.indices)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(data, batch_size = 16, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size = 16, shuffle=False)\n",
        "\n",
        "\"\"\"\n",
        "dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [train_size, valid_size])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 16, shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = 16, shuffle=False)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "33EUKx3NgK8u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cc41db23-cc74-4333-c7d6-9fd11a85d015"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [train_size, valid_size])\\ndataloader = torch.utils.data.DataLoader(dataset, batch_size = 16, shuffle=True)\\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = 16, shuffle=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNN36DN3MFxZ"
      },
      "source": [
        "##Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "QUfeyfpuZ4fj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "07044f01-5ef3-4270-87a6-ec669ec75467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\n",
            "100%|██████████| 254M/254M [00:01<00:00, 237MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-688de8c8b162>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACTIVATION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m             for hook_id, hook in (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segmentation_models_pytorch/encoders/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# Identity and Sequential stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# Block stages need drop_connect rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m             for hook_id, hook in (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/efficientnet_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ],
      "source": [
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "from torchsummary import summary\n",
        "ENCODER = 'efficientnet-b7'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "ACTIVATION = 'sigmoid'\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "model = smp.Unet(\n",
        "    encoder_name = ENCODER,\n",
        "    encoder_weights = ENCODER_WEIGHTS,\n",
        "    in_channels = 3,\n",
        "    classes = 1,\n",
        "    activation = ACTIVATION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYL91DCyMNjs"
      },
      "source": [
        "##Model Train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위쪽은 원래 train 코드와 동일합니다.\n",
        "\n",
        "\n",
        "with torch.no_grad()--- 부분부터는 위에서 나눈 val 데이터 세트를 가져와 검증합니다\n",
        "\n",
        "\n",
        "(이때는 학습을 하지 않습니다.)\n",
        "\n",
        "\n",
        "loss: train에서 사용한 criterion\n",
        "\n",
        "Accuracy는 픽셀별로 비교한 정확도 (정답픽셀/전체픽셀 *100)\n"
      ],
      "metadata": {
        "id": "JMFINhhai3Xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = 1 # 매우 큰 값으로 초기값 가정\n",
        "patience_limit = 5 # 몇 번의 epoch까지 지켜볼지를 결정\n",
        "patience_check = 0 # 현재 몇 epoch 연속으로 loss 개선이 안되는지를 기록"
      ],
      "metadata": {
        "id": "dJB1tWWjzrCD"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89yFx12SMRBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e64c83ed-09ba-4895-c7b8-e35c1717f634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 142/402 [07:29<14:48,  3.42s/it]"
          ]
        }
      ],
      "source": [
        "import segmentation_models_pytorch.utils\n",
        "#%cd /content\n",
        "# loss function과 optimizer 정의\n",
        "criterion = smp.utils.losses.DiceLoss()\n",
        "optimizer = torch.optim.Adam([dict(params=model.parameters(), lr=0.0001),])\n",
        "\n",
        "\n",
        "#model.load_state_dict(torch.load('weights_only.pth'))\n",
        "# training loop\n",
        "result = []\n",
        "correct=0\n",
        "accuracy=0\n",
        "loss1=0.35\n",
        "for epoch in range(10):  # 10 에폭 동안 학습합니다.\n",
        "    model.train()\n",
        "    model.to('cuda')\n",
        "    epoch_loss = 0\n",
        "    for images, masks in tqdm(dataloader):\n",
        "        images = images.float().to(device)\n",
        "        masks = masks.float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\" early stopping\"\"\"\n",
        "    if loss1 > best_loss: # loss가 개선되지 않은 경우\n",
        "        patience_check += 1\n",
        "\n",
        "        if patience_check >= patience_limit: # early stopping 조건 만족 시 조기 종료\n",
        "            break\n",
        "\n",
        "    else: # loss가 개선된 경우\n",
        "        best_loss = loss1\n",
        "        patience_check = 0\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "          model.eval()\n",
        "\n",
        "\n",
        "          for images, masks in tqdm(val_dataloader):\n",
        "              val_images = images.float().to(device)\n",
        "              val_masks = masks.float().to(device) #torch.Size([8, 224, 224])\n",
        "\n",
        "              outputs = model(val_images)\n",
        "              new_masks = outputs.cpu().numpy() #numpy.ndarray 타입변경 (8, 1, 224, 224)\n",
        "              new_masks = np.squeeze(new_masks, axis=1) #(8, 224, 224)\n",
        "              new_masks = (new_masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
        "\n",
        "          for i in range(len(val_images)):\n",
        "              result.append(new_masks[i])\n",
        "\n",
        "          result = torch.tensor(result)\n",
        "          result = result.float().to(device)\n",
        "          loss1 = criterion(result.unsqueeze(1), val_masks.unsqueeze(1))\n",
        "          #확인용\n",
        "          mul1=result*val_masks\n",
        "          mul2=result+val_masks\n",
        "\n",
        "          for i in range(len(val_images)):\n",
        "            correct+= (mul1[i]==1).type(torch.float).sum().item()\n",
        "            correct+= (mul2[i]==0).type(torch.float).sum().item()\n",
        "            accuracy=100* (correct/(224*224*val_masks.shape[0]))\n",
        "\n",
        "#모델 wight 저장\n",
        "#%cd /content/drive/MyDrive/ai project 4-1\n",
        "#torch.save(model.state_dict(), 'weights_only.pth')\n",
        "#%cd /content\n",
        "print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}')\n",
        "print(f'val_dice_score: {loss1},Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "validation 예측 라벨과 정답 라벨을 출력할수 있습니다."
      ],
      "metadata": {
        "id": "o6K50YUAjumX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "figresult=result.to('cpu')\n",
        "figmasks=masks.to('cpu')\n",
        "for i in range(5):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.subplot(131)\n",
        "  plt.imshow(figresult[i])\n",
        "  plt.axis(\"off\")\n",
        "  plt.subplot(132)\n",
        "  plt.imshow(figmasks[i])\n",
        "  plt.axis(\"off\")\n"
      ],
      "metadata": {
        "id": "K4_1zVGGy8-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##model save"
      ],
      "metadata": {
        "id": "75HU_quxj6l7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4MqezeNvXCi"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'weights_only.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqx810IOrlkC"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('weights_only.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jutLc5KNbENg"
      },
      "source": [
        "##test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8EPwknCbHU_"
      },
      "outputs": [],
      "source": [
        "test_dataset = SatelliteDataset(csv_file='./test.csv', transform=transform_test, infer=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf-YBsLYbJax"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    result = []\n",
        "    for images in tqdm(test_dataloader):\n",
        "        images = images.float().to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        masks = outputs.cpu().numpy()\n",
        "        masks = np.squeeze(masks, axis=1)\n",
        "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
        "\n",
        "        for i in range(len(images)):\n",
        "            mask_rle = rle_encode(masks[i])\n",
        "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
        "                result.append(-1)\n",
        "            else:\n",
        "                result.append(mask_rle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhPl-_NZbLl_"
      },
      "source": [
        "##Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "csv파일 저장"
      ],
      "metadata": {
        "id": "CY9_WnjskV15"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npVyzOOobPAu"
      },
      "outputs": [],
      "source": [
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit['mask_rle'] = result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwNWJcpLbPzv"
      },
      "outputs": [],
      "source": [
        "#%cd /content/drive/MyDrive/ai project 4-1\n",
        "submit.to_csv('./submit.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과물 출력"
      ],
      "metadata": {
        "id": "j-1yGYTpkZI4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLL8b1FGZX5X"
      },
      "outputs": [],
      "source": [
        "#%cd /content\n",
        "test_data = pd.read_csv('test.csv')\n",
        "submit2 = pd.read_csv('submit (4).csv')\n",
        "submit3 = pd.read_csv('try4_epoch10 & centercrop & efficientnet & dice loss.csv')\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "for i in range(550,580):\n",
        "  test_image_path = test_data['img_path'][i]\n",
        "  test_image = cv2.imread(test_image_path)\n",
        "  test_mask2 = rle_decode(submit2['mask_rle'][i], (224,224))\n",
        "  test_mask3 = rle_decode(submit3['mask_rle'][i], (224,224))\n",
        "\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.subplot(131)\n",
        "  plt.imshow(test_image)\n",
        "  plt.axis(\"off\")\n",
        "  plt.subplot(132)\n",
        "  plt.imshow(test_mask2)\n",
        "  plt.axis(\"off\")\n",
        "  plt.subplot(133)\n",
        "  plt.imshow(test_mask3)\n",
        "  plt.axis(\"off\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}